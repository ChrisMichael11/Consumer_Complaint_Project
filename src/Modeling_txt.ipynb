{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize, MaxAbsScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Consumer_Complaints_with_Consumer_Complaint_Narratives.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 'df_text' for text modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df_text(df):\n",
    "    df_text = pd.DataFrame()  # Create empty df to fill\n",
    "    \n",
    "    df_text['Consumer complaint narrative'] = df['Consumer complaint narrative']\n",
    "    \n",
    "    cust_resp_dict ={'Closed':0,\n",
    "                 'Untimely response':0,\n",
    "                 'Closed with explanation':1,\n",
    "                 'Closed with non-monetary relief':2,\n",
    "                 'Closed with monetary relief':2}\n",
    "    \n",
    "    df_text['Company response to consumer'] = df['Company response to consumer'].apply(lambda x: cust_resp_dict[x])\n",
    "    \n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company response to consumer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Received Capital One charge card offer XXXX. A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I do n't know how they got my cell number. I t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I 'm a longtime member of Charter One Bank/RBS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After looking at my credit report, I saw a col...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I received a call from a XXXX XXXX from XXXX @...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Consumer complaint narrative  \\\n",
       "0  Received Capital One charge card offer XXXX. A...   \n",
       "1  I do n't know how they got my cell number. I t...   \n",
       "2  I 'm a longtime member of Charter One Bank/RBS...   \n",
       "3  After looking at my credit report, I saw a col...   \n",
       "4  I received a call from a XXXX XXXX from XXXX @...   \n",
       "\n",
       "   Company response to consumer  \n",
       "0                             1  \n",
       "1                             1  \n",
       "2                             1  \n",
       "3                             2  \n",
       "4                             1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_df_text(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split_function(X, y):\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        random_state=11)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(X_train)\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "#     from sklearn.utils import shuffle\n",
    "#     X_shuf, Y_shuf = shuffle(X_transformed, Y)\n",
    "\n",
    "\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english',lowercase=True, min_df=0.001, max_df = 0.2)\n",
    "#     X_train = vectorizer.fit_transform(X_train)\n",
    "#     X_test = vectorizer.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_function(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Kaggle Amazon Food Stuff\n",
    "https://www.kaggle.com/gpayen/d/snap/amazon-fine-food-reviews/building-a-prediction-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "To format our data and build the Term-doc incidence matrix, many operations will be performed on the data:\n",
    "\n",
    "•Stemming\n",
    "•Stop words removal\n",
    "•Lowering\n",
    "•Tokenization\n",
    "•Pruning (numbers and punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #[line.decode('utf-8').strip() for line in title_file.readlines()]\n",
    "    # tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return ' '.join(stems)\n",
    "\n",
    "from string import maketrans\n",
    "intab = string.punctuation\n",
    "outtab = \"                                \"\n",
    "trantab = maketrans(intab, outtab)\n",
    "\n",
    "## Training set\n",
    "corpus = []\n",
    "for text in X_train:\n",
    "    text = text.lower()\n",
    "    text = text.translate(trantab)\n",
    "    text = tokenize(text)\n",
    "    corpus.append(text)\n",
    "    \n",
    "count_vect = CountVectorizer(input=\"file\")\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "## Testing set\n",
    "test_set = []\n",
    "for text in X_test:\n",
    "    text = text.lower()\n",
    "    text = text.translate(trantab)\n",
    "    text.tokenize(text)\n",
    "    test_set.append(text)\n",
    "    \n",
    "X_new_counts = count_vect.transform(test_set)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "from pandas import *\n",
    "prediction = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Another approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
      "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(input='content', lowercase=True, tokenizer=None,\n",
    "                                 stop_words='english', use_idf=True,\n",
    "                                 max_features=50000, ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vect.fit(df['Consumer complaint narrative'])\n",
    "print tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_text_data(df):\n",
    "    from sklearn.preprocessing import StandardScaler, label_binarize, MaxAbsScaler\n",
    "#     print df.head()\n",
    "    y_ = df['Company response to consumer']\n",
    "    y = label_binarize(y_, classes = [0, 1, 2])\n",
    "    n_classes=3 \n",
    "    y = y.sum(axis=1)\n",
    "    df.pop('Company response to consumer')\n",
    "    X = df.values\n",
    "#     X = X.sum(axis=1)\n",
    "\n",
    "#     print X.shape, y.shape\n",
    "#     print y\n",
    "#     print X\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfVectorizer' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-26dc03de702e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_df_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_text_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9cb3fd0532e8>\u001b[0m in \u001b[0;36mcreate_df_text\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create empty df to fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Consumer complaint narrative'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Consumer complaint narrative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     cust_resp_dict ={'Closed':0,\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TfidfVectorizer' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "df_text = create_df_text(tfidf_vect)\n",
    "X, y = prep_text_data(df_text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split_function(X, y):\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        random_state=11)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(X_train)\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "#     from sklearn.utils import shuffle\n",
    "#     X_shuf, Y_shuf = shuffle(X_transformed, Y)\n",
    "\n",
    "\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english',lowercase=True, min_df=0.001, max_df = 0.2)\n",
    "#     X_train = vectorizer.fit_transform(X_train)\n",
    "#     X_test = vectorizer.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_function(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Multinomial Naïve Bayes learning method¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Multinomial'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bernoulli Naïve Bayes learning method¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Bernoulli'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic regression learning method¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "prediction['Logistic'] = logreg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatt(x):\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    return 1\n",
    "vfunc = np.vectorize(formatt)\n",
    "\n",
    "cmp = 0\n",
    "colors = ['b', 'g', 'y', 'm', 'k']\n",
    "for model, predicted in prediction.items():\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n",
    "    cmp += 1\n",
    "\n",
    "plt.title('Classifiers comparaison with ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(set(Score)))\n",
    "    plt.xticks(tick_marks, set(Score), rotation=45)\n",
    "    plt.yticks(tick_marks, set(Score))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, prediction['Logistic'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)    \n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
